{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# microsoft/phi Demo\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model Name (ie. microsoft/phi-1, microsoft/phi-1.5 or microsoft/phi-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model_name = \"microsoft/phi-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb5edb10be94db39aceaed7f6b240da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token = True, use_fast=False, trust_remote_code=True)\n",
    "hf_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "TransformerLens lets you load in 50+ different open source language models,\n",
    "and exposes the internal activations of the model to you. You can cache\n",
    "any internal activation in the model, and add in functions to edit, remove\n",
    "or replace these activations as the model runs.\n",
    "'''\n",
    "input_ids = tokenizer(text, return_tensors='pt')['input_ids'].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Hugging Face outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "TransformerLens lets you load in 50+ different open source language models,\n",
      "and exposes the internal activations of the model to you. You can cache\n",
      "any internal activation in the model, and add in functions to edit, remove\n",
      "or replace these activations as the model runs.\n",
      "\n",
      "TransformerLens is written in Python 3.6, and uses the HuggingFace Transformers\n",
      "library.\n",
      "\n",
      "TransformerLens is released under the MIT License.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import json\n",
      "import logging\n",
      "import argparse\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "from torch.utils.data.distributed import DistributedSampler\n",
      "from torch.utils.data.dataset import Dataset\n",
      "from torch.utils.data.sampler import RandomSampler\n",
      "from torch.utils.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = hf_model.generate(input_ids, max_length=200)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Hugging Face model logits and resid_pre cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hf_outputs = hf_model(input_ids, output_hidden_states=True)\n",
    "    hf_logits_cpu = hf_outputs[\"logits\"].cpu()\n",
    "    hf_resid_pre_cache = hf_outputs[\"hidden_states\"]\n",
    "    hf_resid_pre_cache_cpu = [cache.cpu() for cache in hf_resid_pre_cache]\n",
    "    hf_outputs = hf_model(input_ids, labels=input_ids)\n",
    "    hf_loss_cpu = hf_outputs.loss.cpu()\n",
    "\n",
    "del hf_model\n",
    "del hf_outputs\n",
    "del hf_resid_pre_cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in Hooked Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading model microsoft/phi-2 requires setting trust_remote_code=True\n",
      "WARNING:root:Loading model microsoft/phi-2 state dict requires setting trust_remote_code=True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da90f246c00046a4a67b223c2efd0961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model microsoft/phi-2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hooked_phi = HookedTransformer.from_pretrained(model_name,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        fold_ln=False, \n",
    "                                        fold_value_biases=False, \n",
    "                                        center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Hooked Phi logits and resid_pre cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hooked_phi_logits, hook_phi_cache = hooked_phi.run_with_cache(input_ids)\n",
    "    hooked_phi_loss = hooked_phi(input_ids, return_type='loss')\n",
    "    hooked_phi_loss_cpu = hooked_phi_loss.cpu()\n",
    "    hooked_phi_logits_cpu = hooked_phi_logits.detach().cpu()\n",
    "    hook_phi_cache_cpu = {k: v.cpu() for k, v in hook_phi_cache.items()}\n",
    "    n_layers = hooked_phi.cfg.n_layers\n",
    "\n",
    "del hooked_phi\n",
    "del hooked_phi_logits\n",
    "del hook_phi_cache\n",
    "del hooked_phi_loss\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg logits difference: -4.644679307830302e-08\n",
      "max logits difference: 0.0026350021362304688\n"
     ]
    }
   ],
   "source": [
    "centered_hf_logits = hf_logits_cpu - hf_logits_cpu.mean(-1, keepdim=True)\n",
    "mean_diff = (hooked_phi_logits_cpu - centered_hf_logits).mean()\n",
    "print(\"avg logits difference:\", mean_diff.item())\n",
    "max_diff = (hooked_phi_logits_cpu - centered_hf_logits).abs().max()\n",
    "print(\"max logits difference:\", max_diff.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare resid_pre activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Matching hf and T-Lens residual stream in between transformer blocks *****\n",
      "***** \ttesting with atol=0.01 and rtol=0.01\t *****\n",
      "All layers match with atol=0.01 rtol=0.01\n"
     ]
    }
   ],
   "source": [
    "use_loose_bound = True\n",
    "pass_loose_bound = True\n",
    "print(\"*\"*5, \"Matching hf and T-Lens residual stream in between transformer blocks\", \"*\"*5)\n",
    "# for l in range(hooked_phi.cfg.n_layers):\n",
    "#     try:\n",
    "#         torch.testing.assert_close(hook_phi_cache_cpu[f'blocks.{l}.hook_resid_pre'], \n",
    "#                                    hf_resid_pre_cache_cpu[l], \n",
    "#                                    atol = 1e-5, rtol = 1e-5)\n",
    "#     except:\n",
    "#         max_diff = (hook_phi_cache_cpu[f'blocks.{l}.hook_resid_pre'] - hf_resid_pre_cache_cpu[l]).abs().max()\n",
    "#         print(f\"layer {l} \\t not close, max difference: {max_diff}\")\n",
    "#         use_loose_bound = True\n",
    "\n",
    "if use_loose_bound:\n",
    "    atol = rtol = 1e-2\n",
    "    print(\"*\"*5, f\"\\ttesting with {atol=} and {rtol=}\\t\",\"*\"*5)\n",
    "    for l in range(n_layers):\n",
    "        try:\n",
    "            torch.testing.assert_close(hook_phi_cache_cpu[f'blocks.{l}.hook_resid_pre'], hf_resid_pre_cache_cpu[l], atol=atol, rtol=rtol)\n",
    "        except:\n",
    "            max_diff = (hook_phi_cache_cpu[f'blocks.{l}.hook_resid_pre'] - hf_resid_pre_cache_cpu[l]).abs().max()\n",
    "            print(f\"layer {l} \\t not close, max difference: {max_diff}\")\n",
    "            pass_loose_bound = False\n",
    "\n",
    "    if pass_loose_bound:\n",
    "        print(f\"All layers match with {atol=} {rtol=}\")\n",
    "else: \n",
    "    print(\"All layers match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Lens next token loss: 3.6289827823638916\n",
      "HF next token loss: 3.6289584636688232\n",
      "diff in loss (abs): 2.4318695068359375e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"T-Lens next token loss:\", hooked_phi_loss_cpu.item())\n",
    "print(\"HF next token loss:\", hf_loss_cpu.item())\n",
    "print(\"diff in loss (abs):\", (hf_loss_cpu-hooked_phi_loss_cpu).abs().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
