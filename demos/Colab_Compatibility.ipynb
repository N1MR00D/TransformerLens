{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "import os\n",
    "\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "\n",
    "\n",
    "if IN_COLAB or IN_GITHUB:\n",
    "    # %pip install sentencepiece # Llama tokenizer requires sentencepiece\n",
    "    %pip install transformers>=4.31.0 # Llama requires transformers>=4.31.0 and transformers in turn requires Python 3.8\n",
    "    %pip install torch\n",
    "    %pip install tiktoken\n",
    "    %pip install transformer_lens\n",
    "    %pip install transformers_stream_generator\n",
    "    # !huggingface-cli login --token NEEL'S TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerLens currently supports 181 models out of the box.\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, loading\n",
    "from typing import List\n",
    "import gc\n",
    "\n",
    "untested_models = loading.OFFICIAL_MODEL_NAMES\n",
    "\n",
    "print(\"TransformerLens currently supports \" + str(len(untested_models)) + \" models out of the box.\")\n",
    "\n",
    "GENERATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_models_as_tested(model_set: List[str]) -> None:\n",
    "    for model in model_set:\n",
    "        untested_models.remove(model)\n",
    "    \n",
    "\n",
    "def run_set(model_set: List[str]) -> None:\n",
    "    for model in model_set:\n",
    "        print(\"Testing \" + model)\n",
    "        tl_model = HookedTransformer.from_pretrained_no_processing(model, device=\"cuda\")\n",
    "        if GENERATE:\n",
    "            ptint(tl_model.generate(\"Hello my name is\"))\n",
    "        del tl_model\n",
    "        gc.collect()\n",
    "        if IN_COLAB:\n",
    "            %rm -rf /root/.cache/huggingface/hub/models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following models can run in the T4 free environment\n",
    "free_compatible = [\n",
    "    \"gpt2\",\n",
    "    \"gpt2-medium\",\n",
    "    \"gpt2-large\",\n",
    "    \"gpt2-xl\",\n",
    "    \"distilgpt2\",\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-1.3b\",\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    \"EleutherAI/gpt-neo-1.3B\",\n",
    "    \"EleutherAI/gpt-neo-2.7B\",\n",
    "    \"stanford-crfm/alias-gpt2-small-x21\",\n",
    "    \"stanford-crfm/battlestar-gpt2-small-x49\",\n",
    "    \"stanford-crfm/caprica-gpt2-small-x81\",\n",
    "    \"stanford-crfm/darkmatter-gpt2-small-x343\",\n",
    "    \"stanford-crfm/expanse-gpt2-small-x777\",\n",
    "    \"stanford-crfm/arwen-gpt2-medium-x21\",\n",
    "    \"stanford-crfm/beren-gpt2-medium-x49\",\n",
    "    \"stanford-crfm/celebrimbor-gpt2-medium-x81\",\n",
    "    \"stanford-crfm/durin-gpt2-medium-x343\",\n",
    "    \"stanford-crfm/eowyn-gpt2-medium-x777\",\n",
    "    \"EleutherAI/pythia-14m\",\n",
    "    \"EleutherAI/pythia-31m\",\n",
    "    \"EleutherAI/pythia-70m\",\n",
    "    \"EleutherAI/pythia-160m\",\n",
    "    \"EleutherAI/pythia-410m\",\n",
    "    \"EleutherAI/pythia-1b\",\n",
    "    \"EleutherAI/pythia-1.4b\",\n",
    "    \"EleutherAI/pythia-70m-deduped\",\n",
    "    \"EleutherAI/pythia-160m-deduped\",\n",
    "    \"EleutherAI/pythia-410m-deduped\",\n",
    "    \"EleutherAI/pythia-1b-deduped\",\n",
    "    \"EleutherAI/pythia-1.4b-deduped\",\n",
    "    \"EleutherAI/pythia-70m-v0\",\n",
    "    \"EleutherAI/pythia-160m-v0\",\n",
    "    \"EleutherAI/pythia-410m-v0\",\n",
    "    \"EleutherAI/pythia-1b-v0\",\n",
    "    \"EleutherAI/pythia-1.4b-v0\",\n",
    "    \"EleutherAI/pythia-70m-deduped-v0\",\n",
    "    \"EleutherAI/pythia-160m-deduped-v0\",\n",
    "    \"EleutherAI/pythia-410m-deduped-v0\",\n",
    "    \"EleutherAI/pythia-1b-deduped-v0\",\n",
    "    \"EleutherAI/pythia-1.4b-deduped-v0\",\n",
    "    \"EleutherAI/pythia-160m-seed1\",\n",
    "    \"EleutherAI/pythia-160m-seed2\",\n",
    "    \"EleutherAI/pythia-160m-seed3\",\n",
    "    \"NeelNanda/SoLU_1L_v9_old\",\n",
    "    \"NeelNanda/SoLU_2L_v10_old\",\n",
    "    \"NeelNanda/SoLU_4L_v11_old\",\n",
    "    \"NeelNanda/SoLU_6L_v13_old\",\n",
    "    \"NeelNanda/SoLU_8L_v21_old\",\n",
    "    \"NeelNanda/SoLU_10L_v22_old\",\n",
    "    \"NeelNanda/SoLU_12L_v23_old\",\n",
    "    \"NeelNanda/SoLU_1L512W_C4_Code\",\n",
    "    \"NeelNanda/SoLU_2L512W_C4_Code\",\n",
    "    \"NeelNanda/SoLU_3L512W_C4_Code\",\n",
    "    \"NeelNanda/SoLU_4L512W_C4_Code\",\n",
    "    \"NeelNanda/SoLU_6L768W_C4_Code\",\n",
    "    \"NeelNanda/SoLU_8L1024W_C4_Code\",\n",
    "    \"NeelNanda/SoLU_10L1280W_C4_Code\",\n",
    "    \"NeelNanda/SoLU_12L1536W_C4_Code\",\n",
    "    \"NeelNanda/GELU_1L512W_C4_Code\",\n",
    "    \"NeelNanda/GELU_2L512W_C4_Code\",\n",
    "    \"NeelNanda/GELU_3L512W_C4_Code\",\n",
    "    \"NeelNanda/GELU_4L512W_C4_Code\",\n",
    "    \"NeelNanda/Attn_Only_1L512W_C4_Code\",\n",
    "    \"NeelNanda/Attn_Only_2L512W_C4_Code\",\n",
    "    \"NeelNanda/Attn_Only_3L512W_C4_Code\",\n",
    "    \"NeelNanda/Attn_Only_4L512W_C4_Code\",\n",
    "    \"NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr\",\n",
    "    \"NeelNanda/SoLU_1L512W_Wiki_Finetune\",\n",
    "    \"NeelNanda/SoLU_4L512W_Wiki_Finetune\",\n",
    "    \"Baidicoot/Othello-GPT-Transformer-Lens\",\n",
    "    \"bert-base-cased\",\n",
    "    \"roneneldan/TinyStories-1M\",\n",
    "    \"roneneldan/TinyStories-3M\",\n",
    "    \"roneneldan/TinyStories-8M\",\n",
    "    \"roneneldan/TinyStories-28M\",\n",
    "    \"roneneldan/TinyStories-33M\",\n",
    "    \"roneneldan/TinyStories-Instruct-1M\",\n",
    "    \"roneneldan/TinyStories-Instruct-3M\",\n",
    "    \"roneneldan/TinyStories-Instruct-8M\",\n",
    "    \"roneneldan/TinyStories-Instruct-28M\",\n",
    "    \"roneneldan/TinyStories-Instruct-33M\",\n",
    "    \"roneneldan/TinyStories-1Layer-21M\",\n",
    "    \"roneneldan/TinyStories-2Layers-33M\",\n",
    "    \"roneneldan/TinyStories-Instuct-1Layer-21M\",\n",
    "    \"roneneldan/TinyStories-Instruct-2Layers-33M\",\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigcode/santacoder\",\n",
    "    \"Qwen/Qwen-1_8B\",\n",
    "    \"Qwen/Qwen-1_8B-Chat\",\n",
    "    \"Qwen/Qwen1.5-0.5B\",\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",\n",
    "    \"Qwen/Qwen1.5-1.8B\",\n",
    "    \"Qwen/Qwen1.5-1.8B-Chat\",\n",
    "    \"Qwen/Qwen2-0.5B\",\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2-1.5B\",\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    \"microsoft/phi-1\",\n",
    "    \"microsoft/phi-1_5\",\n",
    "    \"microsoft/phi-2\",\n",
    "    \"ai-forever/mGPT\"\n",
    "]\n",
    "\n",
    "if IN_COLAB:\n",
    "    run_set(free_compatible)\n",
    "    \n",
    "mark_models_as_tested(free_compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_models = [\n",
    "    \"facebook/opt-2.7b\",\n",
    "    \"facebook/opt-6.7b\",\n",
    "    \"EleutherAI/pythia-2.8b\",\n",
    "    \"EleutherAI/pythia-6.9b\",\n",
    "    \n",
    "    \"EleutherAI/pythia-2.8b-deduped\",\n",
    "    \"EleutherAI/pythia-6.9b-deduped\",\n",
    "    \"EleutherAI/pythia-12b-deduped\",\n",
    "    \"EleutherAI/pythia-2.8b-v0\",\n",
    "    \"EleutherAI/pythia-6.9b-v0\",\n",
    "    \"EleutherAI/pythia-12b-v0\",\n",
    "    \"EleutherAI/pythia-2.8b-deduped-v0\",\n",
    "    \"EleutherAI/pythia-6.9b-deduped-v0\",\n",
    "    \"EleutherAI/pythia-12b-deduped-v0\",\n",
    "    \"stabilityai/stablelm-base-alpha-3b\",\n",
    "    \"stabilityai/stablelm-base-alpha-7b\",\n",
    "    \"stabilityai/stablelm-tuned-alpha-3b\",\n",
    "    \"stabilityai/stablelm-tuned-alpha-7b\",\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloom-3b\",\n",
    "    \"bigscience/bloom-7b1\",\n",
    "    \"Qwen/Qwen-7B\",\n",
    "    \"Qwen/Qwen-14B\",\n",
    "    \"Qwen/Qwen-7B-Chat\",\n",
    "    \"Qwen/Qwen-14B-Chat\",\n",
    "    \"Qwen/Qwen1.5-4B\",\n",
    "    \"Qwen/Qwen1.5-4B-Chat\",\n",
    "    \"Qwen/Qwen1.5-7B\",\n",
    "    \"Qwen/Qwen1.5-7B-Chat\",\n",
    "    \"Qwen/Qwen1.5-14B\",\n",
    "    \"Qwen/Qwen1.5-14B-Chat\",\n",
    "    \"Qwen/Qwen2-7B\",\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"google/gemma-2b\",\n",
    "    \"google/gemma-7b\",\n",
    "    \"google/gemma-2b-it\",\n",
    "    \"google/gemma-7b-it\",\n",
    "    \"google/gemma-2-2b\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"google/gemma-2-9b\",\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    \"google/gemma-2-27b\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"01-ai/Yi-6B\",\n",
    "    \"01-ai/Yi-34B\",\n",
    "    \"01-ai/Yi-6B-Chat\",\n",
    "    \"01-ai/Yi-34B-Chat\",\n",
    "]\n",
    "\n",
    "\n",
    "if IN_COLAB:\n",
    "    run_set(paid_models)\n",
    "    \n",
    "mark_models_as_tested(paid_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_models_cpu = [\n",
    "    \"facebook/opt-13b\",\n",
    "    \"facebook/opt-30b\",\n",
    "    \"facebook/opt-66b\",\n",
    "    \"EleutherAI/gpt-neox-20b\",\n",
    "    \"EleutherAI/gpt-j-6B\",\n",
    "    \"EleutherAI/pythia-12b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hosted_models = [\n",
    "    \"ArthurConmy/redwood_attn_2l\",\n",
    "    \"llama-7b-hf\",\n",
    "    \"llama-13b-hf\",\n",
    "    \"llama-30b-hf\",\n",
    "    \"llama-65b-hf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_models = [\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    \"CodeLlama-7b-hf\",\n",
    "    \"CodeLlama-7b-Python-hf\",\n",
    "    \"CodeLlama-7b-Instruct-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-70B\",\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = [\n",
    "    \"Qwen/Qwen-1_8B\",\n",
    "    \"Qwen/Qwen-1_8B-Chat\",\n",
    "    \"google-t5/t5-small\",\n",
    "    \"google-t5/t5-base\",\n",
    "    \"google-t5/t5-large\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "gpt2-medium\n",
      "gpt2-large\n",
      "gpt2-xl\n",
      "distilgpt2\n",
      "facebook/opt-125m\n",
      "facebook/opt-1.3b\n",
      "facebook/opt-2.7b\n",
      "facebook/opt-6.7b\n",
      "facebook/opt-13b\n",
      "facebook/opt-30b\n",
      "facebook/opt-66b\n",
      "EleutherAI/gpt-neo-125M\n",
      "EleutherAI/gpt-neo-1.3B\n",
      "EleutherAI/gpt-neo-2.7B\n",
      "EleutherAI/gpt-j-6B\n",
      "EleutherAI/gpt-neox-20b\n",
      "stanford-crfm/alias-gpt2-small-x21\n",
      "stanford-crfm/battlestar-gpt2-small-x49\n",
      "stanford-crfm/caprica-gpt2-small-x81\n",
      "stanford-crfm/darkmatter-gpt2-small-x343\n",
      "stanford-crfm/expanse-gpt2-small-x777\n",
      "stanford-crfm/arwen-gpt2-medium-x21\n",
      "stanford-crfm/beren-gpt2-medium-x49\n",
      "stanford-crfm/celebrimbor-gpt2-medium-x81\n",
      "stanford-crfm/durin-gpt2-medium-x343\n",
      "stanford-crfm/eowyn-gpt2-medium-x777\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-70m-deduped\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-70m-v0\n",
      "EleutherAI/pythia-160m-v0\n",
      "EleutherAI/pythia-410m-v0\n",
      "EleutherAI/pythia-1b-v0\n",
      "EleutherAI/pythia-1.4b-v0\n",
      "EleutherAI/pythia-2.8b-v0\n",
      "EleutherAI/pythia-6.9b-v0\n",
      "EleutherAI/pythia-12b-v0\n",
      "EleutherAI/pythia-70m-deduped-v0\n",
      "EleutherAI/pythia-160m-deduped-v0\n",
      "EleutherAI/pythia-410m-deduped-v0\n",
      "EleutherAI/pythia-1b-deduped-v0\n",
      "EleutherAI/pythia-1.4b-deduped-v0\n",
      "EleutherAI/pythia-2.8b-deduped-v0\n",
      "EleutherAI/pythia-6.9b-deduped-v0\n",
      "EleutherAI/pythia-12b-deduped-v0\n",
      "EleutherAI/pythia-160m-seed1\n",
      "EleutherAI/pythia-160m-seed2\n",
      "EleutherAI/pythia-160m-seed3\n",
      "NeelNanda/SoLU_1L_v9_old\n",
      "NeelNanda/SoLU_2L_v10_old\n",
      "NeelNanda/SoLU_4L_v11_old\n",
      "NeelNanda/SoLU_6L_v13_old\n",
      "NeelNanda/SoLU_8L_v21_old\n",
      "NeelNanda/SoLU_10L_v22_old\n",
      "NeelNanda/SoLU_12L_v23_old\n",
      "NeelNanda/SoLU_1L512W_C4_Code\n",
      "NeelNanda/SoLU_2L512W_C4_Code\n",
      "NeelNanda/SoLU_3L512W_C4_Code\n",
      "NeelNanda/SoLU_4L512W_C4_Code\n",
      "NeelNanda/SoLU_6L768W_C4_Code\n",
      "NeelNanda/SoLU_8L1024W_C4_Code\n",
      "NeelNanda/SoLU_10L1280W_C4_Code\n",
      "NeelNanda/SoLU_12L1536W_C4_Code\n",
      "NeelNanda/GELU_1L512W_C4_Code\n",
      "NeelNanda/GELU_2L512W_C4_Code\n",
      "NeelNanda/GELU_3L512W_C4_Code\n",
      "NeelNanda/GELU_4L512W_C4_Code\n",
      "NeelNanda/Attn_Only_1L512W_C4_Code\n",
      "NeelNanda/Attn_Only_2L512W_C4_Code\n",
      "NeelNanda/Attn_Only_3L512W_C4_Code\n",
      "NeelNanda/Attn_Only_4L512W_C4_Code\n",
      "NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr\n",
      "NeelNanda/SoLU_1L512W_Wiki_Finetune\n",
      "NeelNanda/SoLU_4L512W_Wiki_Finetune\n",
      "ArthurConmy/redwood_attn_2l\n",
      "llama-7b-hf\n",
      "llama-13b-hf\n",
      "llama-30b-hf\n",
      "llama-65b-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "CodeLlama-7b-hf\n",
      "CodeLlama-7b-Python-hf\n",
      "CodeLlama-7b-Instruct-hf\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "Baidicoot/Othello-GPT-Transformer-Lens\n",
      "bert-base-cased\n",
      "roneneldan/TinyStories-1M\n",
      "roneneldan/TinyStories-3M\n",
      "roneneldan/TinyStories-8M\n",
      "roneneldan/TinyStories-28M\n",
      "roneneldan/TinyStories-33M\n",
      "roneneldan/TinyStories-Instruct-1M\n",
      "roneneldan/TinyStories-Instruct-3M\n",
      "roneneldan/TinyStories-Instruct-8M\n",
      "roneneldan/TinyStories-Instruct-28M\n",
      "roneneldan/TinyStories-Instruct-33M\n",
      "roneneldan/TinyStories-1Layer-21M\n",
      "roneneldan/TinyStories-2Layers-33M\n",
      "roneneldan/TinyStories-Instuct-1Layer-21M\n",
      "roneneldan/TinyStories-Instruct-2Layers-33M\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "bigscience/bloom-560m\n",
      "bigscience/bloom-1b1\n",
      "bigscience/bloom-1b7\n",
      "bigscience/bloom-3b\n",
      "bigscience/bloom-7b1\n",
      "bigcode/santacoder\n",
      "Qwen/Qwen-1_8B\n",
      "Qwen/Qwen-7B\n",
      "Qwen/Qwen-14B\n",
      "Qwen/Qwen-1_8B-Chat\n",
      "Qwen/Qwen-7B-Chat\n",
      "Qwen/Qwen-14B-Chat\n",
      "Qwen/Qwen1.5-0.5B\n",
      "Qwen/Qwen1.5-0.5B-Chat\n",
      "Qwen/Qwen1.5-1.8B\n",
      "Qwen/Qwen1.5-1.8B-Chat\n",
      "Qwen/Qwen1.5-4B\n",
      "Qwen/Qwen1.5-4B-Chat\n",
      "Qwen/Qwen1.5-7B\n",
      "Qwen/Qwen1.5-7B-Chat\n",
      "Qwen/Qwen1.5-14B\n",
      "Qwen/Qwen1.5-14B-Chat\n",
      "Qwen/Qwen2-0.5B\n",
      "Qwen/Qwen2-0.5B-Instruct\n",
      "Qwen/Qwen2-1.5B\n",
      "Qwen/Qwen2-1.5B-Instruct\n",
      "Qwen/Qwen2-7B\n",
      "Qwen/Qwen2-7B-Instruct\n",
      "microsoft/phi-1\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "google/gemma-2b\n",
      "google/gemma-7b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "01-ai/Yi-6B\n",
      "01-ai/Yi-34B\n",
      "01-ai/Yi-6B-Chat\n",
      "01-ai/Yi-34B-Chat\n",
      "google-t5/t5-small\n",
      "google-t5/t5-base\n",
      "google-t5/t5-large\n",
      "ai-forever/mGPT\n"
     ]
    }
   ],
   "source": [
    "# The models listed in the cell below have not been tested\n",
    "print(*untested_models, sep = '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
